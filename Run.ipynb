{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import numpy\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import shutil\n",
    "import socket\n",
    "import string\n",
    "import threading\n",
    "import time\n",
    "import urllib.request\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas\n",
    "import libcloud\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from paramiko.buffered_pipe import PipeTimeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_config import \\\n",
    "    SSH_USER, GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, GCLOUD_PROJECT, IMAGE_NAME, LOCAL_SSH_PUBLIC_KEY_PATH, SSH_KEY_USERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIGN_CSV = 'design.csv'  # The CSV with the experiment design, tab seperated\n",
    "MACHINE_TYPE_MASTER = 'e2-standard-4'\n",
    "MACHINE_TYPE_SLAVES = 'e2-standard-16'\n",
    "NUM_NODES = 3 # amount of slaves\n",
    "BATCH_SIZE = 128\n",
    "EX_RUNTIME = 60 * 60  # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start ComputeEngine Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)\n",
    "location = [l for l in driver.list_locations() if l.id == '2210'][0]\n",
    "# network = [n for n in driver.ex_list_networks() if n.id == '8043342384481294734'][0]\n",
    "\n",
    "ex_id = ''.join(random.choice(string.ascii_lowercase) for i in range(8))  # Generate a random project id\n",
    "print(f\"Experiment ID: {ex_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start MasterNode instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nodes import MasterNode\n",
    "\n",
    "master = MasterNode(driver, location, f\"master-{ex_id}-1\", MACHINE_TYPE_MASTER, master=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read experiment config and put it in the /raw/ direcory\n",
    "experiments = pandas.read_csv(DESIGN_CSV, sep='\\t')\n",
    "experiments.columns.values[0] = 'Index'\n",
    "experiments.set_index('Index')\n",
    "\n",
    "os.makedirs(f\"raw/{ex_id}\", exist_ok=True)\n",
    "shutil.copyfile(DESIGN_CSV, f\"raw/{ex_id}/design.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Slave instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nodes import SlaveNode\n",
    "\n",
    "print(f\"Starting {NUM_NODES} slaves\")\n",
    "slaves = [SlaveNode(driver, location, f\"slave-{ex_id}-{i}\", MACHINE_TYPE_SLAVES, masterNode=master) for i in range(NUM_NODES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in experiments.iterrows():\n",
    "    max_epochs = int(row['max_epochs'])\n",
    "    model = str(row['model'])\n",
    "    executor_memory = int(row['executor_memory'])\n",
    "    executor_cores = int(row['executor_cores'])\n",
    "    total_executor_cores = NUM_NODES * executor_cores # to keep 1 executor per node\n",
    "    print(f\"Experiment {row['Index']}/{ex_id} - epochs: {max_epochs}, model: {model}, ex. mem: {executor_memory}, ex. cores: {executor_cores}, tot. ex. cores: {total_executor_cores}\")\n",
    "\n",
    "    filename = f\"{int(row.Index)}-model{model}-epochs{max_epochs}-executor_memory{executor_memory}-executor_cores{executor_cores}.log\"\n",
    "\n",
    "    stdin, stdout, stderr = master.ssh.exec_command(f\"sudo /home/{SSH_USER}/bd/spark/bin/spark-submit --master spark://{master.privip}:7077 --driver-cores 2 \"\n",
    "                                                    f\"--driver-memory 2G --total-executor-cores {total_executor_cores} --executor-cores {executor_cores} --executor-memory {executor_memory}G \"\n",
    "                                                    f\"--py-files /home/{SSH_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{SSH_USER}/bd/codes/{model}.py \"\n",
    "                                                    f\"--properties-file /home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf \"\n",
    "                                                    f\"--jars /home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \"\n",
    "                                                    f\"--conf spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \"\n",
    "                                                    f\"--conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/codes/{model}.py \"\n",
    "                                                    f\"--action train --dataPath /tmp/mnist --batchSize {BATCH_SIZE * NUM_NODES} --endTriggerNum {max_epochs} \"\n",
    "                                                    f\"--learningRate 0.01 --learningrateDecay 0.0002 > {ex_id}-{filename}\", timeout=EX_RUNTIME)\n",
    "\n",
    "    try:\n",
    "        print(stdout.read())\n",
    "        print(stderr.read())\n",
    "    except PipeTimeout:\n",
    "        print(\"PipeTimeout\")\n",
    "    except socket.timeout:\n",
    "        print(\"Socket timeout\")\n",
    "    sftp = master.ssh.open_sftp()\n",
    "    sftp.get(f'{ex_id}-{filename}', f'raw/{ex_id}/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "FOLDER_NAME = ex_id # Enter the name of the folder to export\n",
    "\n",
    "dir = os.scandir('raw/')\n",
    "experiments = list(filter(lambda x: x.is_dir() and x.name[0] != '.' and x.name == FOLDER_NAME, dir))\n",
    "assert len(experiments) > 0, f'The folder {FOLDER_NAME} does not exist!'\n",
    "\n",
    "def experiment_to_csv(path):\n",
    "    jobs = pd.read_csv(f'{experiment.path}/design.csv', sep='\\t')\n",
    "    jobs.columns.values[0] = 'Index'\n",
    "    jobs = jobs[jobs.Index >= 0]\n",
    "    \n",
    "    jobs['accuracy'] = 0.0\n",
    "    jobs['epochs'] = 0\n",
    "    pat = re.compile(\"accuracy: (\\d+\\.\\d+)\")\n",
    "    pat_clock = re.compile(\"Wall clock time is (\\d+\\.\\d+) ms\")\n",
    "    found, not_existing = 0, 0\n",
    "    for idx, row in jobs.iterrows():\n",
    "        filename = f\"{int(row.Index)}-model{row.model}-epochs{row.max_epochs}-executor_memory{row.executor_memory}-executor_cores{row.executor_cores}.log\"\n",
    "        \n",
    "        try:\n",
    "            with open(f'{experiment.path}/{filename}', 'r') as file:\n",
    "                content = file.read()\n",
    "                result = pat.findall(content)\n",
    "                result_wallclock = float(pat_clock.findall(content)[-1])/1000\n",
    "                jobs.at[idx, 'time'] = result_wallclock\n",
    "                jobs.at[idx, 'accuracy'] = result[-1] if len(result) > 0 else 0\n",
    "                jobs.loc[idx, 'epochs'] = len(result)\n",
    "                found += 1\n",
    "        except FileNotFoundError as e:\n",
    "#             print(e)\n",
    "            jobs.at[idx, 'time'] = -1\n",
    "            jobs.at[idx, 'accuracy'] = -1\n",
    "            jobs.loc[idx, 'epochs'] = -1\n",
    "            not_existing += 1\n",
    "    \n",
    "    time = os.stat(f'{experiment.path}/design.csv').st_mtime\n",
    "    dt = datetime.fromtimestamp(time)\n",
    "    jobs.to_csv(f'ex-{dt.strftime(\"%Y-%m-%d_%H:%M\")}.csv')\n",
    "    return (jobs, f'ex-{dt.strftime(\"%Y-%m-%d_%H:%M\")}.csv', found, not_existing)\n",
    "\n",
    "for experiment in experiments:\n",
    "    if os.path.exists(f'{experiment.path}/design.csv'):\n",
    "        jobs, path, found, not_existing = experiment_to_csv(experiment.path)\n",
    "        print(f'Results of {experiment} stored in {path}')\n",
    "        print(f'{found} of the {found + not_existing} expected log files are present. The other experiments are probably missing.\\n')\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[['max_epochs', 'executor_memory', 'executor_cores', 'model', 'accuracy', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
